# -*- coding: utf-8 -*-
"""Copia de Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nCJrI3Z6osm5WTgSwSOImK6bYaLU4IkY
"""

import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
import matplotlib.pyplot as plt
from collections import Counter
from wordcloud import WordCloud
import string

# Descargar stopwords
nltk.download('stopwords')

nltk.download('punkt')

stop_words = set(stopwords.words('english'))

df = pd.read_csv('/content/sample_data/scopus.csv')
df.head()

grouped = df.groupby('Source title')['Title'].count()
plt.figure(figsize=(20, 10))
grouped.plot(kind='bar')
plt.show()

grouped = df.groupby('Source title')['Title'].count().sort_values(ascending=False)
top_sources = grouped.head(10)
top_sources.plot(kind='bar', color='skyblue')
plt.title('Fuentes con más artículos publicados')
plt.xlabel('Source title')
plt.ylabel('Número de artículos')
plt.show()

# Función para limpiar y preparar texto
def clean_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    return ' '.join([word for word in text.split() if word not in stop_words])

# Aplicar la limpieza de texto a los títulos
df['clean_title'] = df['Title'].apply(clean_text)

# Contar palabras en todos los títulos
word_counts = Counter()
df['clean_title'].str.split().apply(word_counts.update)

# Crear un DataFrame a partir del contador de palabras
word_df = pd.DataFrame(word_counts.items(), columns=['Word', 'Frequency'])

# Ordenar las palabras por frecuencia de aparición, de mayor a menor
word_df = word_df.sort_values(by='Frequency', ascending=False).reset_index(drop=True)

# Añadir columna de rank
word_df['Rank'] = word_df['Frequency'].rank(method='dense', ascending=False)

# Mostrar el DataFrame de palabras ordenadas por rango
word_df.head(20)

# Aplicar la limpieza de texto a los títulos
df['clean_title'] = df['Title'].apply(clean_text)

# Concatenar todos los títulos limpios en un solo texto
text = ' '.join(df['clean_title'])

wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('on')
plt.show()

import matplotlib.pyplot as plt
word_df.sort_values(by='Frequency', ascending=False).head(25).plot(x='Word', y='Frequency', kind='bar')
plt.xlabel('Word')
_ = plt.ylabel('Frequency')

# Contar palabras en todos los títulos
word_counts = Counter()
df['Title'].apply(lambda title: word_counts.update(clean_text(title)))

# Determinar un número de palabras clave a considerar (e.g., top 10)
num_keywords = 15
keywords = [word for word, count in word_counts.most_common(num_keywords)]

# Función para contar cuántas palabras clave contiene cada título
def count_keywords(title):
    words = clean_text(title)
    return sum(word in words for word in keywords)

# Contar las palabras clave por título
df['keyword_count'] = df['Title'].apply(count_keywords)

# Ordenar el DataFrame por 'keyword_count' de forma descendente
df_sorted = df.sort_values(by='keyword_count', ascending=False)

# Graficar los top 10 artículos con más palabras clave
plt.figure(figsize=(10, 5))
plt.bar(df_sorted['Title'].head(15), df_sorted['keyword_count'].head(15), color='skyblue')
plt.xticks(rotation=90)
plt.xlabel('Artículo')
plt.ylabel('Cantidad de palabras clave')
plt.title('Top 10 artículos con mayor cantidad de palabras clave')
plt.tight_layout()  # Ajusta automáticamente los parámetros de la subtrama
plt.show()

# Seleccionar los abstracts de los artículos con más palabras clave
top_abstracts = df.sort_values(by='keyword_count', ascending=False).head(10)['Abstract']

# Función para resumir un abstract
def summarize_abstract(abstract):
    sentences = sent_tokenize(abstract)
    word_freq = FreqDist(clean_text(abstract))
    scores = {sentence: sum(word_freq[word] for word in word_tokenize(sentence.lower()) if word in word_freq) for sentence in sentences}
    summary_sentences = sorted(scores, key=scores.get, reverse=True)[:3]  # Tomar las 3 oraciones con mayor puntuación
    return ' '.join(summary_sentences)

# Aplicar la función de resumen a cada abstract seleccionado
top_abstracts_summary = top_abstracts.apply(summarize_abstract)

# Mostrar resúmenes
for i, summary in enumerate(top_abstracts_summary, 1):
    print(f"Resumen {i}:\n{summary}\n")

# Contar la aparición de palabras clave en cada título
df['keyword_count'] = df['Title'].apply(lambda x: sum(word in clean_text(x) for word in word_df))

# Seleccionar los abstracts de los artículos con más palabras clave
top_articles = df.sort_values(by='keyword_count', ascending=False).head(10)

# Función para resumir un abstract
def summarize_abstract(abstract):
    sentences = sent_tokenize(abstract)
    word_freq = FreqDist(clean_text(abstract))
    scores = {sentence: sum(word_freq[word] for word in word_tokenize(sentence.lower()) if word in word_freq) for sentence in sentences}
    summary_sentences = sorted(scores, key=scores.get, reverse=True)[:2]
    return ' '.join(summary_sentences)

# Aplicar la función de resumen a cada abstract seleccionado y almacenar junto con el título
top_articles['Summary'] = top_articles['Abstract'].apply(summarize_abstract)

# Mostrar títulos y resúmenes
for index, row in top_articles.iterrows():
    print(f"Title: {row['Title']}\nSummary: {row['Summary']}\n")

# Asegurarse de que la columna 'Authors' no tenga valores nulos
df['Authors'] = df['Authors'].fillna('')

# Agrupar por autor y contar el número de artículos
author_counts = df['Authors'].str.split(',').explode().str.strip().value_counts()

# Mostrar los 10 autores más productivos
top_authors = author_counts.head(10)
plt.figure(figsize=(12, 6))  # Ajustar el tamaño de la figura
top_authors.plot(kind='bar', color='skyblue')
plt.title('Autores más productivos')
plt.xlabel('Autores')
plt.ylabel('Número de artículos')
plt.xticks(rotation=45, ha='right')  # Ajustar la rotación y alineación de las etiquetas
plt.tight_layout()  # Ajustar automáticamente los parámetros de la subtrama
plt.show()

# Contar las apariciones de países en las afiliaciones
country_counts = df['Affiliations'].str.findall(r'\b[A-Z][a-z]+\b').explode().value_counts()

# Mostrar los 10 países más frecuentes en las afiliaciones
top_countries = country_counts.head(10)
top_countries.plot(kind='bar', color='skyblue')
plt.title('Países más frecuentes en las afiliaciones')
plt.xlabel('País')
plt.ylabel('Frecuencia')
plt.xticks(rotation=45)
plt.show()

# Convertir la columna 'Year' a datetime
df['Year'] = pd.to_datetime(df['Year'], format='%Y')

# Contar el número de artículos por año
year_counts = df['Year'].dt.year.value_counts().sort_index()

# Mostrar la distribución temporal de publicaciones
year_counts.plot(kind='line', marker='o', color='skyblue')
plt.title('Distribución temporal de publicaciones')
plt.xlabel('Año')
plt.ylabel('Número de artículos')
plt.grid(True)
plt.show()

# Ordenar los artículos por el número de citaciones
top_cited_articles = df.sort_values(by='Cited by', ascending=False).head(10)

# Mostrar los artículos más citados
plt.figure(figsize=(10, 5))
plt.bar(top_cited_articles['Title'], top_cited_articles['Cited by'], color='skyblue')
plt.xticks(rotation=90)
plt.xlabel('Artículo')
plt.ylabel('Número de citaciones')
plt.title('Top 10 artículos más citados')
plt.tight_layout()  # Ajusta automáticamente los parámetros de la subtrama
plt.show()

# Calcular la longitud de los títulos
df['title_length'] = df['Title'].apply(lambda x: len(x.split()))

# Mostrar la distribución de la longitud de los títulos
df['title_length'].plot(kind='hist', bins=20, color='skyblue', edgecolor='black')
plt.title('Distribución de la longitud de los títulos')
plt.xlabel('Número de palabras en el título')
plt.ylabel('Frecuencia')
plt.show()

# Calcular y mostrar la longitud promedio de los títulos
avg_title_length = df['title_length'].mean()
print(f'La longitud promedio de los títulos es: {avg_title_length:.2f} palabras')